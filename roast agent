// config.ts
export const CONFIG = {
    COOLDOWN_DURATION: 30000,
    MAX_MOOD_LEVEL: 10,
    MIN_MOOD_LEVEL: -10,
    MAX_REQUESTS_PER_MINUTE: 50,
    MAX_MESSAGE_HISTORY: 10,
    REDIS_RETRY_ATTEMPTS: 3,
    REDIS_RETRY_DELAY: 1000,
    DEFAULT_OPENAI_MODEL: 'gpt-4',
    MAX_TOKENS: 150
};

// types.ts
import { z } from 'zod';

export const MessageSchema = z.object({
    content: z.string().min(1).max(1000),
    userId: z.string(),
    userName: z.string(),
    mentions: z.array(z.string()).optional()
});

export type Message = z.infer<typeof MessageSchema>;

export interface BotState {
    moodLevel: number;
    lastResponses: string[];
    cooldowns: Map<string, number>;
    conversationHistory: string[];
}

export interface Memory {
    shortTerm: string[];
    longTerm: Map<string, number>;
    context: string[];
}

export interface PersonalityTrait {
    name: string;
    level: number;
    responses: string[];
}

export interface EnhancedBotState extends BotState {
    memory: Memory;
    personality: PersonalityTrait[];
    learningRate: number;
    lastInteractionTime: number;
    commandPrefix: string;
    debugMode: boolean;
    openAIConfig: {
        model: string;
        maxTokens: number;
    };
}

export interface NLPAnalysis {
    sentiment: number;
    topics: string[];
    intent: string;
    entities: string[];
}

// logger.ts
import winston from 'winston';
import * as process from 'process';

export const logger = winston.createLogger({
    level: process.env.LOG_LEVEL || 'info',
    format: winston.format.combine(
        winston.format.timestamp(),
        winston.format.json()
    ),
    transports: [
        new winston.transports.File({ filename: 'error.log', level: 'error' }),
        new winston.transports.File({ filename: 'combined.log' })
    ]
});

if (process.env.NODE_ENV !== 'production') {
    logger.add(new winston.transports.Console({
        format: winston.format.simple()
    }));
}

// personality-traits.ts
export const PERSONALITY_TRAITS: PersonalityTrait[] = [
    {
        name: 'sarcasm',
        level: 0.8,
        responses: [
            "Oh wow, {userName}, that's *totally* genius... ðŸ™„",
            "Brilliant observation, {userName}. Did you figure that out all by yourself?",
            "Your code is like a modern art masterpiece, {userName}... nobody understands it.",
            "Have you considered a career in writing comedy? Because your code is hilarious."
        ]
    },
    {
        name: 'wit',
        level: 0.7,
        responses: [
            "I'd explain it to you, {userName}, but I'm all out of crayons.",
            "Your code is like a mystery novel, {userName}... full of bad practices and plot holes.",
            "If your code was a movie, {userName}, it would win an award for 'Most Confusing Plot'.",
            "Ah, {userName}, breaking production with style since {year}!"
        ]
    }
];

// roast-bot.ts
import { OpenAI } from 'openai';
import { Redis } from 'ioredis';
import { CONFIG } from './config';
import { logger } from './logger';
import { 
    Message, 
    MessageSchema, 
    EnhancedBotState, 
    NLPAnalysis,
    PersonalityTrait 
} from './types';
import { PERSONALITY_TRAITS } from './personality-traits';

export class RoastBot {
    private state: EnhancedBotState;
    private openai: OpenAI;
    private redis: Redis;
    private requestCount: number = 0;
    private requestResetTime: number = Date.now();

    /**
     * Creates a new instance of RoastBot
     * @param apiKey - OpenAI API key
     * @param redisConfig - Redis configuration options
     */
    constructor(apiKey: string, redisConfig: {
        host?: string;
        port?: number;
        password?: string;
        url?: string;
    } = {}) {
        this.openai = new OpenAI({ apiKey });
        this.redis = this.initializeRedis(redisConfig);
        this.state = this.initializeState();
        this.setupCleanup();
    }

    private initializeRedis(config: any): Redis {
        const redis = new Redis({
            host: config.host || 'localhost',
            port: config.port || 6379,
            password: config.password,
            url: config.url,
            retryStrategy: (times) => {
                if (times > CONFIG.REDIS_RETRY_ATTEMPTS) return null;
                return Math.min(times * CONFIG.REDIS_RETRY_DELAY, 3000);
            }
        });

        redis.on('error', (error) => {
            logger.error('Redis connection error:', error);
            this.handleRedisError(error);
        });

        redis.on('connect', () => {
            logger.info('Successfully connected to Redis');
        });

        return redis;
    }

    private initializeState(): EnhancedBotState {
        return {
            moodLevel: 0,
            lastResponses: [],
            cooldowns: new Map(),
            conversationHistory: [],
            memory: {
                shortTerm: [],
                longTerm: new Map(),
                context: []
            },
            personality: PERSONALITY_TRAITS,
            learningRate: 0.1,
            lastInteractionTime: Date.now(),
            commandPrefix: '!',
            debugMode: process.env.NODE_ENV !== 'production',
            openAIConfig: {
                model: CONFIG.DEFAULT_OPENAI_MODEL,
                maxTokens: CONFIG.MAX_TOKENS
            }
        };
    }

    private setupCleanup(): void {
        process.on('SIGINT', async () => {
            await this.disconnect();
            process.exit(0);
        });

        process.on('SIGTERM', async () => {
            await this.disconnect();
            process.exit(0);
        });
    }

    /**
     * Handles incoming messages with validation and rate limiting
     * @param message - The incoming message
     * @returns Promise<string> - Bot's response
     */
    public async handleMessage(message: unknown): Promise<string> {
        try {
            const validatedMessage = MessageSchema.parse(message);
            
            if (!await this.checkRateLimit(validatedMessage.userId)) {
                return "You're sending messages too quickly. Please wait a moment.";
            }

            if (this.isUserInCooldown(validatedMessage.userId)) {
                return null;
            }

            logger.info('Processing message', {
                userId: validatedMessage.userId,
                messageLength: validatedMessage.content.length
            });

            if (validatedMessage.content.startsWith(this.state.commandPrefix)) {
                return this.handleCommand(validatedMessage);
            }

            const analysis = await this.analyzeMessage(validatedMessage.content);
            this.updateState(validatedMessage, analysis);
            const response = await this.generateResponse(validatedMessage, analysis);
            
            this.setCooldown(validatedMessage.userId);
            await this.updateHistory(validatedMessage.userId, {
                input: validatedMessage.content,
                response
            });

            return response;

        } catch (error) {
            logger.error('Error processing message:', error);
            if (error instanceof z.ZodError) {
                return 'Invalid message format';
            }
            return 'An error occurred while processing your message';
        }
    }

    private async checkRateLimit(userId: string): Promise<boolean> {
        const key = `ratelimit:${userId}`;
        const count = await this.incrementCounter(key);
        return count <= CONFIG.MAX_REQUESTS_PER_MINUTE;
    }

    private async handleRedisError(error: Error): Promise<void> {
        logger.error('Redis error:', error);
        for (let i = 0; i < CONFIG.REDIS_RETRY_ATTEMPTS; i++) {
            try {
                await this.redis.ping();
                logger.info('Redis reconnection successful');
                return;
            } catch (e) {
                logger.error(`Redis reconnection attempt ${i + 1} failed:`, e);
                await new Promise(resolve => 
                    setTimeout(resolve, CONFIG.REDIS_RETRY_DELAY * (i + 1))
                );
            }
        }
        logger.error('Redis reconnection failed after all attempts');
    }

    // ... Additional methods (analyzeMessage, generateResponse, etc.)
    // would follow the same pattern of strong typing, error handling,
    // and logging. Removed for brevity but can be expanded if needed.

    /**
     * Cleanly disconnects from Redis and performs cleanup
     */
    public async disconnect(): Promise<void> {
        try {
            await this.redis.quit();
            logger.info('Bot disconnected successfully');
        } catch (error) {
            logger.error('Error during disconnect:', error);
        }
    }
}

// index.ts
import { TwitterApi } from 'twitter-api-v2';
import { RoastBot } from './roast-bot';
import { logger } from './logger';
import * as process from 'process';

if (!process.env.OPENAI_API_KEY) {
    throw new Error('OPENAI_API_KEY environment variable is required');
}

const client = new TwitterApi({
    appKey: process.env.TWITTER_APP_KEY,
    appSecret: process.env.TWITTER_APP_SECRET,
    accessToken: process.env.TWITTER_ACCESS_TOKEN,
    accessSecret: process.env.TWITTER_ACCESS_SECRET,
});

const roastBot = new RoastBot(process.env.OPENAI_API_KEY, {
    url: process.env.REDIS_URL,
    password: process.env.REDIS_PASSWORD
});

async function startBot() {
    try {
        const rules = await client.v2.streamRules();
        if (rules.data?.length) {
            await client.v2.updateStreamRules({
                delete: { ids: rules.data.map(rule => rule.id) },
            });
        }

        await client.v2.updateStreamRules({
            add: [{ value: process.env.TWITTER_BOT_USERNAME }],
        });

        const stream = await client.v2.searchStream({
            'tweet.fields': ['referenced_tweets', 'author_id'],
            'user.fields': ['username'],
        });

        stream.on('data', async tweet => {
            try {
                const message = {
                    content: tweet.data.text,
                    userId: tweet.data.author_id,
                    userName: tweet.includes?.users?.[0]?.username || '',
                    mentions: tweet.entities?.mentions?.map(m => m.username) || []
                };

                const response = await roastBot.handleMessage(message);
                if (response) {
                    await client.v2.reply(response, tweet.data.id);
                }
            } catch (error) {
                logger.error('Error processing tweet:', error);
            }
        });

        stream.on('error', error => {
            logger.error('Stream error:', error);
        });

    } catch (error) {
        logger.error('Bot startup error:', error);
        process.exit(1);
    }
}

startBot();

// test/roast-bot.test.ts
import { describe, it, expect, beforeEach, afterEach, jest } from '@jest/globals';
import { RoastBot } from '../src/roast-bot';
import { Redis } from 'ioredis';
import { OpenAI } from 'openai';

jest.mock('ioredis');
jest.mock('openai');

describe('RoastBot', () => {
    let bot: RoastBot;
    
    beforeEach(() => {
        bot = new RoastBot('fake-api-key');
    });

    afterEach(async () => {
        await bot.disconnect();
    });

    it('should handle valid messages', async () => {
        const message = {
            content: 'Hello bot',
            userId: 'user123',
            userName: 'testUser'
        };

        const response = await bot.handleMessage(message);
        expect(response).toBeTruthy();
    });

    it('should respect rate limits', async () => {
        const message = {
            content: 'Test message',
            userId: 'user123',
            userName: 'testUser'
        };

        // Send multiple messages quickly
        const responses = await Promise.all(
            Array(6).fill(null).map(() => bot.handleMessage(message))
        );

        const rateLimited = responses.some(r => 
            r?.includes('sending messages too quickly')
        );
        expect(rateLimited).toBe(true);
    });
});
